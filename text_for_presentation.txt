I would like to describe my project, which involves creating a book recommendation agent that operates using local inference. For this, I utilized the Ollama LLaMA 3 model and incorporated web search capabilities to enhance its functionality.

I specifically chose to implement a local model because it was crucial for my previous project idea. I wanted to minimize external API calls and experiment with local processing. Initially, I attempted to set up the agent using SmolAgents, but I encountered issues. The setup was overly complex, with the agent taking up to twenty steps without making meaningful progress. While the book extraction from user input worked well, the overall flow of the agent was difficult to control, leading to numerous errors.

To simplify the process, I decided to abandon the SmolAgents approach and instead programmed a straightforward workflow. This involved issuing direct commands, such as "do this" and "then do that," which eliminated the unpredictable actions of the agent. However, I found this setup too simplistic and not challenging enough, as I wanted to explore more advanced agent frameworks.

Consequently, I switched to using LangGraph, which I hoped would provide better control through nodes and edges. After considerable experimentation, I successfully set up the LangGraph framework to run effectively. The three main steps it employs are centered around the core tasks of the agent.

Here’s a breakdown of the system, together with some ideas you could turn into slides or talking points for your presentation.

    Overall Architecture
    • Three‐stage pipeline built as a StateGraph (LangGraph)
    – extract_books → recommend_books → reasoning → END
    • Async flow orchestrated by graph.astream() and driven by user input
    • Gradio front‐end (app.py) binds the async pipeline to a simple web UI

    Key Components
    • agents.py
    – AsyncLogger: thread‐safe, in‐memory log collector
    – extract_books_node: calls LLM (ollama) to extract JSON of {title,author}
    – recommend_books_node: uses duckduckgo_search to fetch web snippets
    – reasoning_node: calls LLM again to winnow & justify final recommendations
    • search.py
    – Async DuckDuckGo HTML scraping via httpx + selectolax
    – Optional logging hook for tracing each fetch
    • app.py
    – Builds & compiles the graph once at startup
    – Exposes a Gradio app: textbox → pipeline → two output boxes

    Data Flow & State
    • StateGraph: each node transforms or augments the “state” dict
    • extract_books_node adds “extracted_books”
    • recommend_books_node adds “recommendations” + “reasoning” (raw)
    • reasoning_node produces “final_recommendations” + “final_reasoning”

    Prompt Engineering
    • Careful JSON‐only prompts for deterministic parsing
    • Two LLM calls: one to extract, one to consolidate & justify
    • Regex/JSON fallback: extract_json_array() to rescue partial outputs

    Asynchronicity & Logging
    • Full async stack: LLM client, HTTP client, graph traversal
    • AsyncLogger ensures logs from all nodes interleave safely
    • Exposes both human‐friendly reasoning and debug log

    UI Layer
    • Gradio Blocks: minimal code to spin up an interactive demo
    • Two outputs: (1) human‐readable recs, (2) raw reasoning & debug

    Error Handling & Resilience
    • If LLM outputs malformed JSON → fall back to empty list + log error
    • If web search yields zero results → continue gracefully
    • Final reasoning always produced, even if no recommendations

    Possible Extensions / Future Work
    • Caching of DuckDuckGo queries for speed & rate‐limit safety
    • More advanced scraper or official API fallback
    • Stateful conversations: let users refine or follow up
    • A/B test different prompt formats or LLM models
    • UI polish: richer card display for each book

    Demo
    • Live walkthrough: type “I like Dune and Foundation” → show logs
    • Inspect intermediate logs to illustrate how state flows

    Lessons Learned
    • State graphs make multi‐step LLM pipelines clear & maintainable
    • Prompt discipline (JSON‐only) reduces brittle parsing
    • Async design keeps I/O non‐blocking under the hood
    • Separation of concerns (extraction vs. search vs. reasoning)

One key lesson I learned was the importance of simplifying tasks for local inference. It proved more effective to break down complex tasks into smaller, manageable steps rather than attempting to execute a single large task. Additionally, I discovered that ensuring the correct conversion of outputs to JSON format was critical for the agent's reasoning process.

Another significant insight was the necessity of understanding how the agent reasons in the background. Without visibility into this process, it was challenging to identify and correct any erroneous outputs. For a long time, I mistakenly believed that the outputs were derived from training data rather than web search results. It wasn't until I made the entire reasoning, acting, and observing process transparent that I could pinpoint the issue: often, a failure to correctly translate results into JSON left the input blank for subsequent steps.

While the SmolAgents framework provided useful output, detailing what occurred at each step, the LangGraph setup required me to code a complete textual description of the background processes. Unfortunately, the basic setup offered no output regarding the internal workings, which was not helpful.

So basically what i achieved was creating a running set up with LangGraph that is able to achieve quite simple goal relatively reliable although only a simple model works in the background.

I definitely wanted a live demo in HF Spaces That took me very long to accomplish beacause of getting my local ollama setup to HuggingFace inference. Finally I got a (slow on CPU) solution with transformers.

The outlook:
On my way to create the OfficeBot I see need for the following:
- create a tool / tools / agent that can recognize an existing File Structure and sort away a recognized dokument (assessment: easy)
- create a tool / tools / agent that understands what follow-ups could result from a given document (e.g. given a document about an expected repayment it should reason that the follow-up task is to check an account if the payment has arrived) (assessment: medium)
- create tools that enable the agent to do these follow-ups (assessment: hard)
- create a tool / tools / agent able to recognize the content of documents (like positions of an invoice) and can transcribe that into an excel sheet or similar program (assessment: hard)