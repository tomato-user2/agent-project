# agents.py
from langgraph.graph import StateGraph, END
from search import duckduckgo_search
import asyncio
import re
import json
import asyncio
import httpx
import os
import ast
from huggingface_hub import InferenceClient

# Create a single shared client
# It will read your HUGGINGFACEHUB_API_TOKEN from the env for authentication
client = InferenceClient()

async def hf_chat(model: str, messages: list[dict]):
    """
    Call the Hugging Face Inference API chat endpoint
    in a thread pool so we can await it in our async graph.
    """
    # inference client is sync, so we offload to a threadpool
    loop = asyncio.get_running_loop()

    def _sync_call():
        return client.chat.completions.create(
            model=model,
            messages=messages,
            # you can pass generation params here too:
            # temperature=0.7, max_new_tokens=512, ...
        )

    # call sync code in executor
    completion = await loop.run_in_executor(None, _sync_call)

    # completion.choices is a list of ChatCompletionChoice
    # each .message has role & content
    return {
        "message": {
            "role": completion.choices[0].message.role,
            "content": completion.choices[0].message.content
        }
    }

# Alias `chat` to your HF-backed version
chat = hf_chat

class AsyncLogger:
    def __init__(self):
        self._log = []
        self._lock = asyncio.Lock()
    
    async def log(self, message):
        async with self._lock:
            self._log.append(message)
    
    async def get_log(self):
        async with self._lock:
            return "\n".join(self._log)
    
    async def clear(self):
        async with self._lock:
            self._log.clear()

logger = AsyncLogger()

def extract_json_array(text: str):
    # Remove Markdown/HTML formatting
    text = re.sub(r"```(?:json)?\n?|</?(?:pre|code|p)>", "", text, flags=re.IGNORECASE)

    # Extract the first [...] block
    match = re.search(r"(\[\s*{.*?}\s*\])", text, re.DOTALL)
    if not match:
        return []
    
    json_str = match.group(1)

    # Try parsing as JSON
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print("[extract_json_array] JSON decode error:", e)

        # Fallback: try ast.literal_eval
        try:
            return ast.literal_eval(json_str)
        except Exception as e2:
            print("[extract_json_array] literal_eval failed:", e2)
            return []

# Node 1: Extract books from user input
async def extract_books_node(state):
    try:
        print("[extract_books_node] üëâ enter")
        # await logger.clear()
        user_input = state.get("user_input", "")
        prompt = (
            "Extract all book titles and authors from the user input. Do not add books on your own, just take the user input."
            "If a book is mentioned but the author is missing, fill it in using your knowledge. "
            "ONLY output a JSON list of dicts, like this:\n"
            '[{"title": "...", "author": "..."}, ...]\n'
            "Do not add any explanations, prefixes, or markdown. Just the JSON list.\n\n"
            f"User input: {user_input}"
        )
        response = await chat(
            model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
            messages=[{"role":"user","content": prompt}]
        )
        content = response["message"]["content"]

        print("[extract_books_node] LLM raw response:", content)
        # await logger.log(f"[extract_books_node] LLM response: {content}")

        books = extract_json_array(content)

        print("[extract_books_node] Extracted books:", books)
        print("[extract_books_node] üëà exit with", result)
        return {"extracted_books": books}

    except Exception as e:
            print("[extract_books_node] ‚ùå exception:", repr(e))
            raise

# Node 2
async def recommend_books_node(state):
    try:
        print("[recommend_books_node] üëâ enter")
        extracted_books = state.get("extracted_books", [])
        reasoning_steps = []
        recommended_books = []

        print("[recommend_books_node] Extracted books:", extracted_books)
        # await logger.log(f"[recommend_books_node] Extracted books: {extracted_books}")

        if not extracted_books:
            reasoning_steps.append("No books extracted from the input. Check if the extraction failed.")
            return {"recommendations": [], "reasoning": "\n".join(reasoning_steps)}

        for book in extracted_books:
            title = book.get("title", "")
            author = book.get("author", "")
            query = f"Books similar to '{title}' by {author}"
            reasoning_steps.append(f"Searching DuckDuckGo with query: {query}")

            print(f"[recommend_books_node] Searching with query: {query}")
            # await logger.log(f"Searching DuckDuckGo with query: {query}")

            search_results = await duckduckgo_search(query)

            if not search_results:
                reasoning_steps.append(f"No results found for: {query}")
                print(f"[recommend_books_node] No results found for query: {query}")
                # await logger.log(f"No results found for query: {query}")
                continue

            print(f"[recommend_books_node] Results for query '{query}': {search_results}")

            for res in search_results:
                recommended_books.append({
                    "title": res.get("title", "No Title"),
                    "link": res.get("link", ""),
                    "snippet": res.get("snippet", "")
                })
                reasoning_steps.append(f"‚úÖ Found: {res.get('title', 'No Title')} ({res.get('link', '')})")

        if not recommended_books:
            reasoning_steps.append("No recommendations found across all queries.")

        print("[recommend_books_node] Final recommendations:", recommended_books)
        print("[recommend_books_node] üëà exit with", result)
        return {
            "recommendations": recommended_books,
            "reasoning": "\n".join(reasoning_steps)
        }
    
    except Exception as e:
        print("[extract_books_node] ‚ùå exception:", repr(e))
        raise

# Node 3: Reason about the search results and generate recommendations
async def reasoning_node(state):
    recommendations = state.get("recommendations", [])
    initial_reasoning = state.get("reasoning", "")
    
    if not recommendations:
        final_reasoning = initial_reasoning + "\nNo recommendations found to reason about."
        return {"final_recommendations": [], "final_reasoning": final_reasoning}
    
    # Format recommendations as input for the LLM
    recommendations_text = "\n".join(
        [f"Title: {rec['title']}\nLink: {rec['link']}\nSnippet: {rec['snippet']}\n" for rec in recommendations]
    )
    
    prompt = (
    "You are a helpful book recommendation expert. You are given a web search result. "
    "Analyze it and select the most relevant book recommendations. Explain why you recommend each book. "
    "Output only a JSON list like this:\n"
    '[{"title": "...", "reason": "...", "link": "..."}, ...]\n\n'
    "Do not add any explanations, comments, or extra text. Only output the JSON list.\n\n"
    f"Books found from search:\n{recommendations_text}"
)

    
    response = await chat(
        model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
        messages=[{"role":"user","content": prompt}]
    )
    
    content = response['message']['content']

    print("[reasoning_node] LLM raw response:", content)
    # await logger.log(f"[reasoning_node] LLM response: {content}")

    # Extract JSON-like structure
    final_recommendations = extract_json_array(content)

    # Combine previous reasoning with the final reasoning
    final_reasoning = initial_reasoning + "\n\nFinal reasoning:\n"
    for rec in final_recommendations:
        final_reasoning += f"‚úÖ Recommended: {rec.get('title', 'Unknown')} - {rec.get('reason', 'No reason provided.')}\n"

    print("[reasoning_node] Final recommendations extracted:", final_recommendations)
    print("[reasoning_node] Final reasoning:\n", final_reasoning)
    # await logger.log(f"[reasoning_node] Final recommendations extracted: {final_recommendations}")
    # await logger.log(f"[reasoning_node] Final reasoning:\n{final_reasoning}")

    return {
        "final_recommendations": final_recommendations,
        "final_reasoning": final_reasoning
    }


# Build the graph
def build_graph():
    graph = StateGraph(dict)

    graph.add_node("extract_books", extract_books_node)
    graph.add_node("recommend_books", recommend_books_node)
    graph.add_node("reasoning", reasoning_node)

    # Define edges
    graph.add_edge("extract_books", "recommend_books")
    graph.add_edge("recommend_books", "reasoning")
    graph.add_edge("reasoning", END)

    graph.set_entry_point("extract_books")
    return graph.compile()
----------------------------------------

# app.py
import gradio as gr
from agents import build_graph

graph = build_graph()

async def run_book_recommender(user_input):
    # 1) Kick off the state‚Äêgraph via .astream
    initial_state = {"user_input": user_input}
    final_state = None

    try:
        async for state in graph.astream(initial_state):
            final_state = state
    except Exception as e:
        # Log it somewhere if you want
        print("üî• Exception while streaming graph:", e)
        # And then re-raise so Gradio can show it
        raise

    # 2) If for some bizarre reason the graph yielded zero times,
    #    fall back to a safe default
    if final_state is None:
        final_state = {
            "final_recommendations": [],
            "final_reasoning": "‚ö†Ô∏è Graph never yielded a final state."
        }

    # 3) Extract the real outputs
    recs = final_state.get("final_recommendations", [])
    reasoning = final_state.get("final_reasoning", "")

    # 4) Format them
    recs_text = "\n\n".join(
        f"üìò {r['title']}\nüîó {r.get('link','')}\nüí° {r.get('reason','')}"
        for r in recs
    ) or "No recommendations found."

    # 5) **Explicitly return** in all cases
    return recs_text, reasoning

with gr.Blocks() as demo:
    gr.Markdown("# üìö AI Book Recommender")
    user_in = gr.Textbox(label="Tell me some books you like")
    btn = gr.Button("Get Recommendations")
    out_recs = gr.Textbox(label="Recommended Books", lines=10)
    out_reason = gr.Textbox(label="Reasoning / Debug Log", lines=15)

    btn.click(
      fn=run_book_recommender,
      inputs=user_in,
      outputs=[out_recs, out_reason],
    )

if __name__=="__main__":
    demo.launch()

----------------------------------------

# concatenate_py_files.py
import os

def concatenate_py_files(output_file):
    # Get the current directory
    current_directory = os.getcwd()
    
    # List all .py files in the current directory
    py_files = [f for f in os.listdir(current_directory) if f.endswith('.py')]
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for py_file in py_files:
            # Write the file name as a header
            outfile.write(f"# {py_file}\n")
            # Write the content of the file
            with open(py_file, 'r', encoding='utf-8') as infile:
                outfile.write(infile.read())
            # Add a separator line
            outfile.write("\n" + "-"*40 + "\n\n")

if __name__ == "__main__":
    output_file_name = "concatenated_output.txt"
    concatenate_py_files(output_file_name)
    print(f"All .py files have been concatenated into {output_file_name}.")

----------------------------------------

# search.py
# search.py (modify to accept logger)
import httpx
from selectolax.parser import HTMLParser

async def duckduckgo_search(query, max_results=5, logger=None):
    url = f"https://html.duckduckgo.com/html/?q={query}"
    headers = {"User-Agent": "Mozilla/5.0"}
    async with httpx.AsyncClient() as client:
        response = await client.get(url, headers=headers, timeout=10)

    html = HTMLParser(response.text)
    results = []

    for result in html.css("div.result")[:max_results]:
        title_el = result.css_first("a.result__a")
        snippet_el = result.css_first(".result__snippet")

        if title_el and snippet_el:
            title = title_el.text(strip=True)
            link = title_el.attributes.get("href", "")
            snippet = snippet_el.text(strip=True)
            results.append({"title": title, "link": link, "snippet": snippet})

    return results

----------------------------------------

