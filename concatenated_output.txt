# agent.py
from smolagents import ToolCallingAgent, LiteLLMModel, DuckDuckGoSearchTool
from tools import extract_books, recommend_similar_books

# Local Llama3 via Ollama
model = LiteLLMModel(
    model_id="ollama/llama3",
    api_base="http://localhost:11434"
)

agent = ToolCallingAgent(
    tools=[extract_books, recommend_similar_books],
    model=model,
    stream_outputs=True,   # optional real-time output
)
----------------------------------------

# app.py
import gradio as gr
from agent import agent

def chat_with_agent(user_input, chat_history):
    response = agent.run(user_input)
    chat_history.append((user_input, response))
    return chat_history, chat_history

with gr.Blocks() as demo:
    gr.Markdown("## ðŸ“š Book Recommendation Agent (powered by LLaMA3 + smolagents)")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Tell me a few books you like...", label="Your favorite books")
    submit = gr.Button("Submit")  # Create a submit button
    clear = gr.Button("Clear")

    # Connect the submit button to the chat_with_agent function
    submit.click(chat_with_agent, [msg, chatbot], [chatbot, chatbot])
    
    # Keep the existing functionality for the Enter key
    msg.submit(chat_with_agent, [msg, chatbot], [chatbot, chatbot])
    
    clear.click(lambda: ([], ""), None, [chatbot, msg])

demo.launch()

----------------------------------------

# concatenate_py_files.py
import os

def concatenate_py_files(output_file):
    # Get the current directory
    current_directory = os.getcwd()
    
    # List all .py files in the current directory
    py_files = [f for f in os.listdir(current_directory) if f.endswith('.py')]
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for py_file in py_files:
            # Write the file name as a header
            outfile.write(f"# {py_file}\n")
            # Write the content of the file
            with open(py_file, 'r', encoding='utf-8') as infile:
                outfile.write(infile.read())
            # Add a separator line
            outfile.write("\n" + "-"*40 + "\n\n")

if __name__ == "__main__":
    output_file_name = "concatenated_output.txt"
    concatenate_py_files(output_file_name)
    print(f"All .py files have been concatenated into {output_file_name}.")

----------------------------------------

# tools.py
from smolagents.tools import tool
import re
import requests
import logging
from bs4 import BeautifulSoup
import json

# Optional: Configure logging
logging.basicConfig(level=logging.INFO)

def extract_json_array(text):
    """
    Extract the first JSON array from a text string,
    ignoring any leading or trailing non-JSON text.
    """
    # Try to find a JSON array with a regex (non-greedy)
    match = re.search(r"\[\s*\{.*?\}\s*\]", text, re.DOTALL)
    if match:
        candidate = match.group(0)
        try:
            return json.loads(candidate)
        except json.JSONDecodeError as e:
            logging.error(f"JSON decode error on regex candidate: {e}")
    # Fallback: take substring from first [ to last ]
    start = text.find('[')
    end = text.rfind(']')
    if start != -1 and end != -1 and end > start:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError as e:
            logging.error(f"JSON decode error on fallback candidate: {e}")
    raise ValueError("No valid JSON array found in text")

def call_llm(prompt: str, model: str = "ollama/llama3", api_base: str = "http://localhost:11434") -> str:
    """
    Calls a local LLM (e.g., via Ollama HTTP API) with a prompt.

    Args:
        prompt (str): The input prompt for the LLM.
        model (str): The model to use, e.g., "ollama/llama3".
        api_base (str): The base URL of the model endpoint.

    Returns:
        str: The generated text from the model.
    """
    try:
        response = requests.post(
            f"{api_base}/api/generate",
            json={"model": model.replace("ollama/", ""), "prompt": prompt, "stream": False},
            timeout=30,
        )
        response.raise_for_status()
        return response.json()["response"]
    except Exception as e:
        logging.error(f"[call_llm] Failed to call LLM: {e}")
        return ""


def duckduckgo_search_snippets(query: str, max_snippets: int = 3) -> list:
    """
    Performs a DuckDuckGo search and extracts text snippets from the results.
    """
    logging.debug(f"[duckduckgo_search_snippets] Query: {query}")
    try:
        resp = requests.get("https://lite.duckduckgo.com/lite/", params={"q": query}, timeout=10)
        if not resp.ok:
            return [f"[Error] Search failed for query: {query}"]

        soup = BeautifulSoup(resp.text, "html.parser")
        results = soup.find_all("a", class_="result-link")[:max_snippets]
        snippets = []

        for link in results:
            snippet = link.get_text(strip=True)
            if snippet:
                snippets.append(snippet)

        return snippets or ["[No results found]"]
    except Exception as e:
        logging.error(f"[duckduckgo_search_snippets] Failed: {e}")
        return [f"Search error: {e}"]


@tool
def extract_books(text: str) -> list:
    """
    Extracts book titles and authors from user input.

    Args:
        text: Free-form user input describing books they liked.

    Returns:
        A list of dictionaries with:
            - 'title': Title of the book
            - 'author': Author of the book (if known or added via search)
    """
    logging.debug(f"[extract_books] Input: {text}")

    prompt = f"""Extract all books and their authors from this text:

{text}

Return a JSON list like this:
[
  {{
    "title": "Book Title",
    "author": "Author Name"
  }},
  ...
]
"""

    try:
        response = call_llm(prompt)
        # Try to extract JSON block from response
        match = re.search(r"\[\s*{.*?}\s*\]", response, re.DOTALL)
        if match:
            cleaned_json = match.group(0)
            parsed = json.loads(cleaned_json)
            if isinstance(parsed, list):
                return parsed
        raise ValueError("No valid JSON array found")
    except Exception as e:
        logging.error(f"[extract_books] Failed: {e}\nLLM response: {response}")
        return [{"title": "Unknown", "author": "Unknown"}]


@tool
def recommend_similar_books(book_list: list[dict]) -> list[dict]:
    """
    Given a list of books, search the web and suggest similar books.

    Args:
        book_list (list of dict): A list of dictionaries, each with:
            - 'title' (str): The title of the book.
            - 'author' (str): The author of the book.

    Returns:
        list of dict: A list of recommended books. Each dictionary contains:
            - 'title' (str): Title of the recommended book.
            - 'author' (str): Author of the recommended book.
            - 'reason' (str): Explanation of why it was recommended.
    """

    all_snippets = ""
    for book in book_list:
        query = f"Books similar to '{book['title']}' by {book.get('author', 'unknown author')}"
        snippets = duckduckgo_search_snippets(query)
        all_snippets += f"### Search results for {book['title']}:\n" + "\n".join(snippets) + "\n\n"

    prompt = f"""You are a book recommendation assistant.

Here are search results for books similar to user favorites:

{all_snippets}

Based on these, recommend 3 books. For each, include:
- title
- author (if known)
- reason for recommendation (based on search result info)

Return as a JSON list like this:
[
  {{
    "title": "...",
    "author": "...",
    "reason": "..."
  }},
  ...
]
"""

    response = call_llm(prompt, model="ollama/llama3", api_base="http://localhost:11434")

    try:
        recommendations = extract_json_array(response)
        if not isinstance(recommendations, list):
            raise ValueError("Parsed JSON is not a list")
        return recommendations
    except Exception as e:
        logging.error(f"[recommend_similar_books] Failed to parse LLM response: {e}\nResponse was:\n{response}")
        # Return a safe fallback to avoid crashing
        return [{"title": "Unknown", "author": "Unknown", "reason": "Failed to parse LLM output."}]

----------------------------------------

